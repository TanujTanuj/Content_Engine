{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bigsl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bigsl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTENT EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    uber = fitz.open(pdf_path)\n",
    "    google = fitz.open(pdf_path)\n",
    "    tesla = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len([uber,google,tesla])):\n",
    "        page = uber.load_page(page_num)\n",
    "        page = google.load_page(page_num)\n",
    "        page = tesla.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"uber.pdf\")\n",
    "pdf_text = extract_text_from_pdf(\"google.pdf\")\n",
    "pdf_text = extract_text_from_pdf(\"tesla.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Program to measure the similarity between \n",
    "# two sentences using cosine similarity. \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# X = input(\"Enter first string: \").lower() \n",
    "# Y = input(\"Enter second string: \").lower() \n",
    "X =\"believes\"\n",
    "Y =\"expects\"\n",
    "\n",
    "# tokenization \n",
    "X_list = word_tokenize(X) \n",
    "Y_list = word_tokenize(Y) \n",
    "\n",
    "# sw contains the list of stopwords \n",
    "sw = stopwords.words('english') \n",
    "l1 =[];l2 =[] \n",
    "\n",
    "# remove stop words from the string \n",
    "X_set = {w for w in X_list if not w in sw} \n",
    "Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "# form a set containing keywords of both strings \n",
    "rvector = X_set.union(Y_set) \n",
    "for w in rvector: \n",
    "\tif w in X_set: l1.append(1) # create a vector \n",
    "\telse: l1.append(0) \n",
    "\tif w in Y_set: l2.append(1) \n",
    "\telse: l2.append(0) \n",
    "c = 0\n",
    "\n",
    "# cosine formula \n",
    "for i in range(len(rvector)): \n",
    "\t\tc+= l1[i]*l2[i] \n",
    "cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "print(\"similarity: \", cosine) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type dpr to instantiate a model of type rag. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Config has to be initialized with question_encoder and generator config",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2c16d6d69fa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRagRetriever\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m retriever = RagRetriever.from_pretrained(\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"facebook/dpr-ctx_encoder-single-nq-base\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"document\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"compressed\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32mD:\\New folder (2)\\New folder\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, retriever_name_or_path, indexed_dataset, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretriever_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexed_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"datasets\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"faiss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"config\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mRagConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretriever_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m         \u001b[0mrag_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRagTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretriever_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[0mquestion_encoder_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrag_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion_encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\New folder (2)\\New folder\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    608\u001b[0m             )\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\New folder (2)\\New folder\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[1;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"attn_implementation\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"attn_implementation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pruned_heads\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\New folder (2)\\New folder\\lib\\site-packages\\transformers\\models\\rag\\configuration_rag.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_size, is_encoder_decoder, prefix, bos_token_id, pad_token_id, eos_token_id, decoder_start_token_id, title_sep, doc_sep, n_docs, max_combined_length, retrieval_vector_size, retrieval_batch_size, dataset, dataset_split, index_name, index_path, passages_path, use_dummy_dataset, reduce_loss, label_smoothing, do_deduplication, exclude_bos_score, do_marginalize, output_retrieved, use_cache, forced_eos_token_id, dataset_revision, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         )\n\u001b[1;32m--> 127\u001b[1;33m         assert (\n\u001b[0m\u001b[0;32m    128\u001b[0m             \u001b[1;34m\"question_encoder\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"generator\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         ), \"Config has to be initialized with question_encoder and generator config\"\n",
      "\u001b[1;31mAssertionError\u001b[0m: Config has to be initialized with question_encoder and generator config"
     ]
    }
   ],
   "source": [
    "# To load the default \"document\" dataset with 3 pdfs from document (index name is 'compressed' or 'exact')\n",
    "from transformers import RagRetriever\n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/dpr-ctx_encoder-single-nq-base\", dataset=\"document\", index_name=\"compressed\"\n",
    ")\n",
    "\n",
    "# To load your own indexed dataset built with the datasets library. More info on how to build the indexed dataset in examples/rag/use_own_knowledge_dataset.py\n",
    "from transformers import RagRetriever\n",
    "\n",
    "dataset = (\n",
    "    ...\n",
    ")  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", indexed_dataset=dataset)\n",
    "\n",
    "# To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py\n",
    "from transformers import RagRetriever\n",
    "\n",
    "dataset_path = \"path/to/my/dataset\"  # dataset saved via *dataset.save_to_disk(...)*\n",
    "index_path = \"path/to/my/index.faiss\"  # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    index_name=\"custom\",\n",
    "    passages_path=dataset_path,\n",
    "    index_path=index_path,\n",
    ")\n",
    "\n",
    "# To load the legacy index built originally for Rag's paper\n",
    "from transformers import RagRetriever\n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", index_name=\"legacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0431487134d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpytextrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_lg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"textrank\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "example_text = \"\"\"We, like other internet, technology, and media companies, are frequently subject to litigation based on allegations\n",
    "of infringement or other violations of intellectual property rights, including patent, copyright, trade secrets, and\n",
    "trademarks. Parties have also sought broad injunctive relief against us by filing claims in U.S. and international courts\n",
    "and the U.S. International Trade Commission (ITC) for exclusion and cease-and-desist orders. In addition, patentholding companies may frequently seek to generate income from patents they have obtained by bringing claims\n",
    "against us. As we continue to expand our business, the number of intellectual property claims against us has increased\n",
    "and may continue to increase as we develop and acquire new products, services, and technologies.\n",
    "Adverse results in any of these lawsuits may include awards of monetary damages, costly royalty or licensing\n",
    "agreements (if licenses are available at all), or orders limiting our ability to sell our products and services in the U.S. or\n",
    "elsewhere, including by preventing us from offering certain features, functionalities, products, or services in certain\n",
    "jurisdictions. They may also cause us to change our business practices in ways that could result in a loss of revenues\n",
    "for us and otherwise harm our business.\n",
    "Many of our agreements with our customers and partners, including certain suppliers, require us to defend\n",
    "against certain intellectual property infringement claims and in some cases indemnify them for certain intellectual\n",
    "property infringement claims against them, which could result in increased costs for defending such claims or\n",
    "significant damages if there were an adverse ruling in any such claims. Such customers and partners may also\n",
    "discontinue the use of our products, services, and technologies, as a result of injunctions or otherwise, which could\n",
    "result in loss of revenues and harm our business. Moreover, intellectual property indemnities provided to us by our\n",
    "suppliers, when obtainable, may not cover all damages and losses suffered by us and our customers arising from\n",
    "intellectual property infringement claims. Furthermore, in connection with our divestitures, we have agreed, and may in\n",
    "the future agree, to provide indemnification for certain potential liabilities, including those associated with intellectual\n",
    "property claims. Regardless of their merits, intellectual property claims are often time consuming and expensive to\n",
    "litigate or settle. To the extent such claims are successful, they could harm our business, including our product and\n",
    "service offerings, financial condition, and operating results.\"\"\"\n",
    "print('Original Document Size:',len(example_text))\n",
    "doc = nlp(example_text)\n",
    "\n",
    "for sent in doc._.textrank.summary(limit_phrases=2, limit_sentences=2):\n",
    "\tprint(sent)\n",
    "\tprint('Summary Length:',len(sent))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Restarting with windowsapi reloader\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder (2)\\New folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload_files():\n",
    "    uploaded_files = request.files.getlist(\"files\")\n",
    "    # Process files, extract text, compare, and generate insights\n",
    "    return render_template('results.html', results=results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
